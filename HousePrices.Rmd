---
title: "House data"
author: Arrolag
date: "September 11, 2016"
output: 
  html_document: 
    css: C:/Users/user/Desktop/kcc/hw1.css
---


##Introduction
The purpose of this case study is to predict house prices in King County, USA, using various predictors as found in the kcc.csv data set.

## Data Source

The data set contains house-sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.

It was downloaded from https://www.kaggle.com/harlfoxem/housesalesprediction


## Description of the dataset

The dataset comprises of 21613 entries and 21 variables.
The dependent variable is price. 

```{r,comment=NA,message=FALSE}
options(scipen = 999)
house <- read.csv("C:/Users/user/Desktop/Project/kc_house_data.csv", header = TRUE)
str(house)
```

The following are descriptions of various attributes:

* sqft_living, the total house square footage of the house
*	sqft_basement, size of the basement
*	sqft_above = sqft_living - sqft_basement
*	sqft_lot, lot size of the house
*	sqft_living15, the average house square footage of the 15 closest houses
*	sqft_lot15, the average lot square footage of the 15 closest houses



### Data cleaning


*date* is broken down into year, month and day of pricing. The three variables are then converted into factors.

*waterfront* is converted into a factor with 2 levels, 0 (No Waterfront) and 1 (Waterfront). 

*view* is converted into a factor variable with 5 levels: 0, 1, 2, 3, 4.

*condition* is converted into a factor variable with 5 levels: 1, 2, 3, 4, 5.

*grade* is converted into a factor variable with 12 levels: 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13.

From *yr_built* we calculate the *age* of the house from year of sale.

From *yr_renovated* we derive a factor variable *renovated* with 2 levels: 0- not renovated, 1- renovated.

```{r,comment=NA,message=FALSE,warning=FALSE}
require(lubridate)
require(dplyr)
house$date_year <- year(ymd_hms(house$date))
house <- mutate(house,age = date_year - yr_built)
house$date_year <- as.factor(house$date_year)
house$date_month <- as.factor(month(ymd_hms(house$date)))
house$date_day <- as.factor(day(ymd_hms(house$date)))
house$waterfront <- as.factor(house$waterfront)
house$view <- as.factor(house$view)
house$condition <- as.factor(house$condition)
house$grade <- as.factor(house$grade)
fac_fn <- function(x){
    if (x==0)
        return(0)
    else 
        return(1)
}
house$renovated <- sapply(house$yr_renovated,fac_fn)
```



The data set is randomly divided into a training and testing set in the ratio 7:3

```{r,comment=NA,message=FALSE}
dt = sort(sample(nrow(house), nrow(house)*.7))
train<-house[dt,]
test<-house[-dt,]
```

We then perform an analysis of variables within the training set

### Variable analysis

94 house Ids appear twice. This means that these houses were resold during the given sales period. This data could be used later to determine factors that encourage resale of houses in a specified period.

```{r}
table(subset(data.frame(table(train$id)), Freq > 1)$Freq)
```

#### Analysis of linear relationship between pairs of quantitative variables

We modify the correlation matrix (as given by R), so that the 1's on the leading diagonal are replaced by the standard deviations for each variable.
The upper triangle of the matrix is also replaced with the spearman's correlation.


From the correlation matrix, the variables *sqft_living* and *bathrooms* have reasonably high correlations with the dependent variable, *price*. There seems to be almost no correlation between each of *sqft_lot15*, *age* and the dependent variable, *price*.


```{r,comment=NA,message=FALSE}
train_reg <- select(train, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, sqft_living15, sqft_lot15,date_year, date_month, date_day, age, renovated)
num_columns <- c("price","sqft_living","sqft_lot","bedrooms","bathrooms","floors","sqft_above","sqft_basement","sqft_living15","sqft_lot15","age")
corr_a <- cor(train_reg[,num_columns])
corr_pear <- cor(train_reg[,num_columns],method = "spearman")
final_mat <- diag(sapply(train_reg[,num_columns],sd))+upper.tri(corr_pear)*corr_pear +lower.tri(corr_a)*corr_a
round(final_mat,digits = 2)
```


Among the predictors, the highest correlation is to be found between *sqft_above* and *sqft_living* (0.88). Other reasonably high correlations to look out for are between, *sqft_living* and *bathrooms* (0.75), *sqft_living* and *sqft_living15* (0.76), *sqft_lot* and *sqft_lot15* (0.72), *sqft_above* and *bathrooms* (0.68), *sqft_above* and *sqft_living15* (0.73). *age* is negatively correlated with all variables except for *sqft_basement*. We study the effect of these correlations on the regression going forward.

```{r,comment=NA,message=FALSE}
par(mfrow=c(2,5))
for(i in 1:10) {
    hist(train_reg[,num_columns][,i], main=names(train_reg[,num_columns])[i])
}
```

From the histograms and boxplots, we observe that *price*, *sqft_living*, *sqft_lot*, *sqft_above*, *sqft_basement*, *sqft_living15* and *sqft_lot15* have many outliers to the right, hence are highly positively skewed.

```{r,comment=NA, message=FALSE}
par(mfrow=c(2,5))
for(i in 1:10) {
    boxplot(train_reg[,num_columns][,i], main=names(train_reg[,num_columns])[i])
}
```


We note that the response variable is heavily skewed to the right. The effect of this can be seen on the diagnostic plots done on the model containing only the continuous variables *fita*

```{r,comment=NA,message=FALSE}
cont_columns <-c("price","sqft_living","sqft_lot","sqft_above","sqft_basement","sqft_living15","sqft_lot15","age")
fita <- lm(price~., data = train_reg[,cont_columns])
par(mfrow=c(2,2))
plot(fita)
```




Further, plots of the residuals of *fita* against each one of the continuous predictors together with lowess curves are drawn. From this, we observe that *sqft_living*, *sqft_lot*, *sqft_basement* and *sqft_lot15* do not exhibit linearity. To improve this fit, we perform a logarithmic transformation on the dependent variable.

```{r,comment=NA, message=FALSE}
par(mfrow=c(2,3))
for(i in 2:7) {
    plot(train_reg[,cont_columns[i]], fita$residuals, main = names(train_reg[,cont_columns])[i])
    lines(lowess(train_reg[,cont_columns[i]], fita$residuals))
}
cont_columnsb <-c("log_price","sqft_living","sqft_lot","sqft_above","sqft_basement","sqft_living15","sqft_lot15","age")
train_reg$log_price <- log(train_reg$price)
test$log_price <- log(test$price)
fitb <- lm(log_price~., data = train_reg[,cont_columnsb])
par(mfrow=c(2,2))
plot(fitb)
```


QQ plots of the transformed model *fitb* reveal normal residuals. The lowess fit line in the residuals vs fitted values plot also does not show a large deviation from the zero-line. This means that the transformed model better specifies the relationship between the variables.


```{r,comment=NA, message=FALSE}
par(mfrow=c(2,3))
for(i in 2:7) {
    plot(train_reg[,cont_columnsb[i]], fitb$residuals, main = names(train_reg[,cont_columnsb])[i])
    lines(lowess(train_reg[,cont_columnsb[i]], fitb$residuals))
}
```



We perform a regression using all variables, except, *id*, *date*, *zipcode*, *lat*, *long*, *date_day* and *date_month* as predictors for *price*.

From the full model *fitc*, we observe that *renovated* and *sqft_basement* are not strongly related to *price*. We use backward/forward stepwise regression to select the final minimal model which has an adjusted R-squared of 0.6617.


```{r,comment=NA,message=FALSE}
require(stats)
options(show.signif.stars = F)
fitc <- lm(log_price~.-price-date_day-date_month, data = train_reg)
summary(fitc)
null=lm(log_price~1, data=train_reg)
final_model <- step(null, scope = list(upper=fitc), data = train_reg, direction="both")
summary(final_model)
```


We use the final model to predict outcomes in the *test* data. We modify test data factors, that might have additional levels not found in the train data, to give NA values for affected items. Later, these items are ommited to calculate how well the model predicts items in the test data.
The correlation between the actual values and the predicted values is 83.78%.


```{r,comment=NA, message=FALSE}
test$grade <- factor(test$grade, levels= final_model$xlevels$grade) 
preds <- exp(predict(final_model, newdata=test))
actuals_preds <- data.frame(cbind(actuals=test$price, predicteds=preds))
actuals_preds <- na.omit(actuals_preds)
cor(actuals_preds)
```


Next, we will aim to explore possible interactions within the data set for inclusion.
It will also be useful to use other regression methods such as ridge and lasso, and compare the results.

